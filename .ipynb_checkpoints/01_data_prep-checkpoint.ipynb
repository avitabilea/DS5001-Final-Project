{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93487bc1-6d19-42ee-a512-25e1006ef907",
   "metadata": {},
   "source": [
    "# Metadata\n",
    "\n",
    "```yaml\n",
    "Course:   DS5001: Exploratory Text Analytics\n",
    "Topic:    Final Project, Data Prep\n",
    "Author:   Andrew Avitabile\n",
    "Date:     24 March 2024 (Edited April 25, 2024)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b05f642-d52d-4b07-94ee-f87a29b2a4ab",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a0c0b9-d1c2-449f-83fd-c8961b34b441",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a5dc02c-114c-47a8-9ba6-783214eec4d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "#nltk packages\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#Sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Downloading necessary data from nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "# Creating a list of stop words for later use\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize Porter Stemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a9173f3-c8ce-4c7d-a8b4-ae0f66b745ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the base path\n",
    "base_path = \"C:/Users/Andre/Box/DS5001 Final Project/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c0fc8e-506e-44a8-a72b-bdb8ce9c2170",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afd4b495-d3f3-4925-8ead-7a9fa22916e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define filepaths\n",
    "file_path_eval_text = base_path + \"Data/eval_text.xlsx\"\n",
    "\n",
    "# Read the CSV file\n",
    "eval_text = pd.read_excel(file_path_eval_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31b8e8e7-d265-4004-9496-269dafa56ba5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Get just PST feedback. Replace missing feedback with blank strings.\n",
    "eval_text['overallcomments'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b58335-a282-4ed5-990c-3ee8687edd4d",
   "metadata": {},
   "source": [
    "# Parse Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864bc5b9-77d8-4f37-85da-f6a7df1e501a",
   "metadata": {},
   "source": [
    "## Initial Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80d72962-c9c3-4605-9e6f-9128749ba679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adding a 'document_id' column that is the row number starting from 1\n",
    "eval_text = eval_text.reset_index()\n",
    "eval_text['document_id'] = range(1, len(eval_text) + 1)\n",
    "eval_text.set_index('document_id', inplace=True)\n",
    "\n",
    "# Counting documents written by each supervisor\n",
    "n_documents = eval_text.groupby('supervisor').size()\n",
    "\n",
    "# Counting PSTs evaluated by each supervisor\n",
    "n_psts = eval_text.groupby('supervisor')['uin_deident'].nunique()\n",
    "\n",
    "# Joining counts back to the eval_text on supervisor\n",
    "eval_text = eval_text.join(n_documents.rename('n_documents'), on='supervisor')\n",
    "eval_text = eval_text.join(n_psts.rename('n_psts'), on='supervisor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83e54f7-2ae0-4305-b00b-2a90972478ee",
   "metadata": {},
   "source": [
    "## Creating CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7ef6e9b-8a98-42ce-a96e-bece0e9702ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CountVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Tokenize with SciKitLearn\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m engine \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mfit_transform(eval_text\u001b[38;5;241m.\u001b[39moverallcomments)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "#Tokenize with SciKitLearn\n",
    "engine = CountVectorizer()\n",
    "model = engine.fit_transform(eval_text.overallcomments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca7fb9f-e1a6-4dda-b806-24a155395476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple POS grouping function\n",
    "def pos_group(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return 'NOUN'\n",
    "    elif tag.startswith('V'):\n",
    "        return 'VERB'\n",
    "    elif tag.startswith('J'):\n",
    "        return 'ADJECTIVE'\n",
    "    elif tag.startswith('R'):\n",
    "        return 'ADVERB'\n",
    "    else:\n",
    "        return 'OTHER'\n",
    "\n",
    "# Initialize the list to collect token data\n",
    "long_format_data = []\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for document_id, row in eval_text.iterrows():\n",
    "    document = row['overallcomments']\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    for sentence_num, sentence in enumerate(sentences):\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        tagged_tokens = nltk.pos_tag(tokens)  # Get POS tags for the tokens\n",
    "        for token_num, (token, tag) in enumerate(tagged_tokens):\n",
    "            long_format_data.append({\n",
    "                'document_id': document_id,\n",
    "                'sentence_num': sentence_num + 1,\n",
    "                'token_num': token_num + 1,\n",
    "                'token_str': token.lower(),  # Typically, terms are stored in lower case\n",
    "                'term_str': token,           # Original token as it appears\n",
    "                'pos': tag,                  # POS tag\n",
    "                'pos_group': pos_group(tag)  # Grouped POS tag\n",
    "            })\n",
    "\n",
    "# Create DataFrame from long-format data\n",
    "CORPUS = pd.DataFrame(long_format_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41efcc1-a5e3-4f90-ba77-f596e15d16e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f53f4c1-672e-41f3-a3e1-e80824a0ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS.to_csv(base_path + \"output/CORPUS.csv\", sep='|', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe241786-39a8-49e3-8968-72d2393dc611",
   "metadata": {},
   "source": [
    "## Creating LIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c270976a-37cc-4ed4-92c0-39c8ef12dd98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating the new DataFrame LIB from eval_text\n",
    "LIB = eval_text[['supervisor', 'uin_deident', 'order_alt', 'n_documents', 'n_psts', 'overallcomments']].copy()\n",
    "\n",
    "# Count the number of sentences per document\n",
    "sentence_counts = CORPUS.groupby('document_id')['sentence_num'].nunique().rename('sentence_count')\n",
    "\n",
    "# Count the number of tokens per document\n",
    "token_counts = CORPUS.groupby('document_id')['token_num'].size().rename('token_count')\n",
    "\n",
    "# Combine sentence and token counts into a single DataFrame\n",
    "doc_counts = pd.DataFrame({'sentence_count': sentence_counts, 'token_count': token_counts})\n",
    "\n",
    "#Merge LIB with document count information\n",
    "LIB = LIB.join(doc_counts)\n",
    "\n",
    "# Get a count of the characters in the comments\n",
    "LIB['char_count'] = eval_text['overallcomments'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1eb0ef-9342-4d0d-b9b8-b32d850fd64e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14caa85f-52e2-404c-a585-c23fb8fff339",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LIB['char_count'].fillna(0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b97fd0f-a0fa-4eb3-9aee-793e5970c2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB.to_csv(base_path + \"output/LIB.csv\", sep='|', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8969c322-1d01-4890-ab44-35f533f1dff2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating Sentence-Level Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcaa40a-213f-46f7-82f2-4254ff3e41e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Aggregate terms to form sentences and count terms\n",
    "grouped = CORPUS.groupby(['document_id', 'sentence_num'])\n",
    "SENTENCES = pd.DataFrame({\n",
    "    'sentence': grouped['term_str'].apply(' '.join),\n",
    "    'term_count': grouped['term_str'].size()\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten the column multi-levels generated by agg\n",
    "SENTENCES.columns = ['document_id', 'sentence_num', 'sentence', 'term_count']\n",
    "\n",
    "SENTENCES.set_index(['document_id', 'sentence_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdbb8db-8860-4ccf-9b68-0d7001a275b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SENTENCES.to_csv(base_path + \"output/SENTENCES.csv\", sep='|', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380c68d4-56ff-4e86-bc46-a77c271ee143",
   "metadata": {},
   "source": [
    "## Creating VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ca3a0c-a63d-48e8-9225-66e31ec86392",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate Term Frequency across the corpus\n",
    "CORPUS['term_str'] = CORPUS['token_str'].str.lower()  # normalize to lowercase\n",
    "TF = CORPUS['term_str'].value_counts().rename('n')\n",
    "\n",
    "# Calculate Document Frequency\n",
    "DF = CORPUS.groupby('term_str')['document_id'].nunique().rename('df')\n",
    "\n",
    "# Calculate IDF using log scaling\n",
    "total_documents = CORPUS['document_id'].nunique()\n",
    "IDF = np.log(total_documents / DF).rename('idf')\n",
    "\n",
    "# Calculate DFIDF\n",
    "DFIDF = (DF * IDF).rename('dfidf')\n",
    "\n",
    "# Stemming and identifying stopwords\n",
    "VOCAB = pd.DataFrame(index=TF.index)\n",
    "VOCAB['n'] = TF\n",
    "VOCAB['df'] = DF\n",
    "VOCAB['idf'] = IDF\n",
    "VOCAB['dfidf'] = DFIDF\n",
    "VOCAB['porter_stem'] = VOCAB.index.map(lambda x: stemmer.stem(x))\n",
    "VOCAB['stop'] = VOCAB.index.isin(stop_words)\n",
    "\n",
    "# Get max POS and POS group for each term\n",
    "max_pos = CORPUS.groupby('term_str')['pos'].agg(lambda x: x.value_counts().idxmax()).rename('max_pos')\n",
    "max_pos_group = CORPUS.groupby('term_str')['pos_group'].agg(lambda x: x.value_counts().idxmax()).rename('max_pos_group')\n",
    "\n",
    "VOCAB = VOCAB.join(max_pos)\n",
    "VOCAB = VOCAB.join(max_pos_group)\n",
    "\n",
    "# Assuming handling of ngrams if applicable\n",
    "# Here we assume unigram as example; modify if you have actual ngrams data\n",
    "VOCAB['ngram_length'] = VOCAB.index.map(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad30e919-d908-4b26-8566-d55cbee834a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dd186c-2297-4eb9-8415-4bce84f62642",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VOCAB.to_csv(base_path + \"output/VOCAB.csv\", sep='|', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857f38c4-52cb-4be7-aa17-941b3c7bde03",
   "metadata": {},
   "source": [
    "### Top 20 most significant words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5c20f1-019a-4712-8d17-78d0193c04f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sorting by DFIDF to find the top 20 significant words\n",
    "top_20_significant = VOCAB.sort_values(by='dfidf', ascending=False).head(20)\n",
    "top_20_significant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
