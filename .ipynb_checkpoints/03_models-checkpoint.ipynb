{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b09fcb2-c1d2-4e14-9119-3467e4f90224",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Metadata\n",
    "\n",
    "```yaml\n",
    "Course:   DS5001: Exploratory Text Analytics\n",
    "Topic:    Final Project, Models\n",
    "Author:   Andrew Avitabile\n",
    "Date:     24 March 2024 (Edited April 25, 2024)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b811323d-4810-4aff-a48e-c87a16ab3254",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f64726-94ca-43fb-8d0d-317453b808b6",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "857200da-4106-4c8f-887b-c77638d704e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#BERT transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce95542-6c7c-4b6e-b92f-df20f655cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base path\n",
    "base_path = \"C:/Users/Andre/Box/DS5001 Final Project/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf5110e-c86f-4f86-ad41-90e2eb103f5c",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3bb406-12d7-4bb4-97b3-04054d654434",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB = pd.read_csv(base_path + \"output/LIB.csv\", delimiter = \"|\")\n",
    "CORPUS = pd.read_csv(base_path + \"output/CORPUS.csv\", delimiter = \"|\")\n",
    "VOCAB = pd.read_csv(base_path + \"output/TFIDF_L2.csv\", delimiter = \"|\")\n",
    "BOW_sentence = pd.read_csv(base_path + \"output/BOW_sentence.csv\", sep='|', index=True)\n",
    "TFIDF = pd.read_csv(base_path + \"output/TFIDF_L2.csv\", delimiter = \"|\")\n",
    "TFIDF_L2 = pd.read_csv(base_path + \"output/TFIDF_L2.csv\", delimiter = \"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e888c83-3e84-45ef-9787-2286854a61be",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7466499-8aff-4a44-a856-e8bd66dd2f27",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8285ea48-ae5d-4891-8824-5c4093594419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Number of PCA components\n",
    "n_components = 5\n",
    "\n",
    "# Initialize PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "principal_components = pca.fit_transform(TFIDF_L2)\n",
    "\n",
    "# Create a DataFrame with the principal components\n",
    "principal_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5'])\n",
    "\n",
    "# Getting the PCA components (i.e., eigenvectors)\n",
    "components = pca.components_\n",
    "\n",
    "# Relate components to original features\n",
    "terms = TFIDF.columns\n",
    "PCA_COMPONENTS = pd.DataFrame(components, columns=terms, index=[f'PC{i+1}' for i in range(5)])\n",
    "PCA_COMPONENTS = PCA_COMPONENTS.T\n",
    "PCA_COMPONENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb552fa-0f32-4bb9-a7a7-fe975150cbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PCA\n",
    "PCA = PCA(n_components=n_components)\n",
    "\n",
    "# Fit PCA on the normalized TFIDF data and transform the data\n",
    "document_components = pca.fit_transform(TFIDF_L2.values)\n",
    "\n",
    "# Create a DataFrame for the document-component matrix\n",
    "PCA_DCM = pd.DataFrame(document_components, index=TFIDF_L2.index, columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "\n",
    "# Merge with LIB\n",
    "PCA_DCM = LIB.merge(PCA_DCM, left_index=True, right_index=True)\n",
    "PCA_DCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77ed113-ae5d-4ce5-9f92-55266cfff54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Scatterplot of documents\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=PCA_DCM['PC1'],\n",
    "    y=PCA_DCM['PC2'],\n",
    "    mode='markers',\n",
    "    \n",
    "    text=PCA_DCM['overallcomments'],  # Assuming you want to see comments on hover\n",
    "    name='Documents'\n",
    "))\n",
    "\n",
    "# Update layout with titles and labels\n",
    "fig.update_layout(\n",
    "    title='PCA Analysis: Documents and Loadings',\n",
    "    xaxis_title='Component 1',\n",
    "    yaxis_title='Component 2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a68d5ac-86a2-45d0-bc93-38c27268d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "# Scatterplot of documents\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=PCA_DCM['PC2'],\n",
    "    y=PCA_DCM['PC3'],\n",
    "    mode='markers',\n",
    "    \n",
    "    text=PCA_DCM['overallcomments'],  # Assuming you want to see comments on hover\n",
    "    name='Documents'\n",
    "))\n",
    "\n",
    "# Update layout with titles and labels\n",
    "fig.update_layout(\n",
    "    title='PCA Analysis: Documents and Loadings',\n",
    "    xaxis_title='Component 2',\n",
    "    yaxis_title='Component 3'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751d79b5-fb28-4fe2-aa98-cc0dca6c13df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(PCA_COMPONENTS['PC1'], PCA_COMPONENTS['PC2'], color='blue', alpha=0.5)\n",
    "plt.title('PCA - Loadings Plot on First Two Principal Components')\n",
    "plt.xlabel('Principal Component 1 Loadings')\n",
    "plt.ylabel('Principal Component 2 Loadings')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3e489a-e94c-4647-9639-d5b9a97790a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(PCA_COMPONENTS['PC2'], PCA_COMPONENTS['PC3'], color='blue', alpha=0.5)\n",
    "plt.title('PCA - Loadings Plot on First Two Principal Components')\n",
    "plt.xlabel('Principal Component 2 Loadings')\n",
    "plt.ylabel('Principal Component 3 Loadings')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0abc554-1777-4b69-a8cc-f2c8f859ba6e",
   "metadata": {},
   "source": [
    "## Topic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efaa8ef-a488-4857-a69b-725b9609efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out tokens with undesired POS tags including all forms of punctuation and cardinal numbers\n",
    "unwanted_tags = ['NNP', 'PRP', 'PRP$', 'WP', 'WP$', '.', ',', ':', '``', \"''\", '(', ')', '#', 'CD', '$'] \n",
    "\n",
    "# Apply the filter to the DataFrame\n",
    "CORPUS_LIM = CORPUS[~CORPUS['pos'].isin(unwanted_tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80171c31-aab4-484a-a97e-e6a74e5e12ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(CORPUS_LIM['term_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb09697-4331-48a5-9d60-063b3c535fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "lda.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f69ecfc-7d72-48c2-9f3d-4c00b76523dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic-Term Distribution (PHI)\n",
    "phi_df = pd.DataFrame(lda.components_, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Document-Topic Distribution (THETA)\n",
    "theta_df = pd.DataFrame(lda.transform(dtm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff3ed55-9e94-423f-aeab-5a096f6b7e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "display_topics(lda, vectorizer.get_feature_names_out(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aaf35d-8f6c-4f86-91d2-cda211ce138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "def get_sentiment(text):\n",
    "    # Encode the text using tokenizer\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    # Get model output\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "    # Calculate probabilities using softmax\n",
    "    probabilities = softmax(output.logits, dim=1)\n",
    "    # Extract scores for each sentiment class\n",
    "    sentiment_scores = probabilities.numpy().flatten()\n",
    "    labels = ['negative', 'neutral', 'positive']\n",
    "    # Create a dictionary of labels and their corresponding scores\n",
    "    sentiment_result = dict(zip(labels, sentiment_scores))\n",
    "    return sentiment_result\n",
    "\n",
    "# Step 2: Apply sentiment analysis\n",
    "SENTENCES['sentiment'] = SENTENCES['sentence'].apply(get_sentiment)\n",
    "\n",
    "# Step 3: Optionally merge this back with the original dataframe if needed\n",
    "CORPUS_W_SENT = pd.merge(CORPUS.drop(columns='term_str'), SENTENCES, on=['document_id', 'sentence_num'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362a3a9f-c832-404a-b52c-2ef942473983",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f8595-e1fa-485c-bed3-d5d6e770f667",
   "metadata": {
    "tags": []
   },
   "source": [
    "### VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f54754b-b9d0-49d0-b138-de26d320b089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "#Define a function to get the compund sentiment score\n",
    "def get_sentiment(term):\n",
    "    score = sia.polarity_scores(term)\n",
    "    return score['compound']  # Return the compound score for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c4537c-c011-4843-a848-0b94715e7631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the function to the VOCAB table\n",
    "VOCAB['sentiment'] = VOCAB.index.map(get_sentiment)\n",
    "\n",
    "#Look at non-zero instances\n",
    "VOCAB.query('sentiment != 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53e38e5-7b6b-424e-a2e7-bf77263f362b",
   "metadata": {},
   "source": [
    "### BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5a975b-1970-4860-82dc-de2fd89d8469",
   "metadata": {},
   "source": [
    "## Word Embeddings (word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d192ff-1cda-4e2d-b490-542bf73d1b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.manifold import TSNE as tsne\n",
    "import plotly_express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df23f569-4f3c-4926-9734-8be11990482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS = CORPUS.set_index(['document_id', 'sentence_num', 'token_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b319208d-86c3-48a4-b672-ead5aa9cf54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BAG = 'document_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc88a94-e4cd-44b0-95f7-fb60daf4a483",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Dictionary(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c544c035-a326-4a35-9fee-e654d92b29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gensim-style document\n",
    "docs = CORPUS[~CORPUS.pos.str.match('NNPS?')].dropna(subset=['term_str'])\\\n",
    "    .groupby(BAG)\\\n",
    "    .term_str.apply(lambda  x:  x.tolist())\\\n",
    "    .reset_index()['term_str'].tolist()\n",
    "docs = [doc for doc in docs if len(doc) > 1] # Lose single word docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5fe0f5-93a1-4e1c-9625-93376654d60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec parameters\n",
    "w2v_params = dict(\n",
    "    window = 5,\n",
    "    vector_size = 246,\n",
    "    min_count = 50, # THIS LIMITS OUR VOCAB\n",
    "    workers = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3c5ab3-734a-4a8f-80a1-ec355468e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(docs, **w2v_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b9206a-9068-4d34-8d28-70739aa52dfe",
   "metadata": {},
   "source": [
    "### Visualize with tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47f0d2c-93bd-4c2c-94e4-2f9abeff2c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(row):\n",
    "    w = row.name\n",
    "    try:\n",
    "        vec = model.wv[w]\n",
    "    except KeyError as e:\n",
    "        vec = None\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6abb64f-e656-48ab-805b-91786348dbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "WV = pd.DataFrame(VOCAB.apply(get_vector, axis=1).dropna()).apply(lambda x: pd.Series(x[0]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ad80dc-ba74-4ced-a448-84a61725a72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "WV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9449d13f-5ea1-456b-924b-37c89624cc35",
   "metadata": {},
   "source": [
    "### Use ScikitLearn's TSNE library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36cf6ad-845d-44fa-ad78-d86ecc1c9a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_engine = tsne(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab15677-b102-4fb7-bbbc-d6001b559b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_model = tsne_engine.fit_transform(WV.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4663c52b-cbd1-4209-ba45-8589320e1692",
   "metadata": {},
   "outputs": [],
   "source": [
    "TSNE = pd.DataFrame(tsne_model, columns=['x','y'], index=WV.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f48db6c-026b-4986-819c-14021895af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a53d8-a5ea-4e52-be80-a986abc724e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out tokens with undesired POS tags including all forms of punctuation and cardinal numbers\n",
    "unwanted_tags = ['NNP', 'PRP', 'PRP$', 'WP', 'WP$', '.', ',', ':', '``', \"''\", '(', ')', '#', 'CD', '$'] \n",
    "\n",
    "X = TSNE.join(VOCAB[~VOCAB['max_pos'].isin(unwanted_tags)], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba22fd01-9ef2-4be6-beba-0c24ef751a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(X.reset_index(), 'x', 'y', \n",
    "           text='term_str', \n",
    "           color='max_pos', \n",
    "           hover_name='term_str',          \n",
    "           size='dfidf',\n",
    "           height=1000).update_traces(\n",
    "                mode='markers+text', \n",
    "                textfont=dict(color='black', size=14, family='Arial'),\n",
    "                textposition='top center')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
